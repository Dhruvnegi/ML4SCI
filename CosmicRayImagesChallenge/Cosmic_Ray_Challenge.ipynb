{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Cosmic_Ray_Challenge.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhihegde001/ML4SCI-1/blob/main/CosmicRayImagesChallenge/Cosmic_Ray_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-C40d9p6Bqg"
      },
      "source": [
        "# Hackathon - Cosmic Ray Detection Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs8EcFUP6Bql"
      },
      "source": [
        "## 1. Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpp_qFW7XTsH"
      },
      "source": [
        "### 1.1 Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1kqKmSc6Bqo"
      },
      "source": [
        "- CCD-based cameras are used extensively in exploration satellites and space telescopes for imaging the surfaces of celestial bodies, and deep-space objects such as stars, extrasolar systems, and galaxies. \n",
        "\n",
        "- These sensors are often subjected to constant irradiation by galactic cosmic rays. The interaction of these ionizing radiations with optical sensors leads to confusion and loss of imaging pixels (an ‘artifact’).\n",
        "\n",
        "- Your goal is to develop a model which automatically locates cosmic ray artifacts by creating bounding boxes around them\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOg1fH2R6Bqt"
      },
      "source": [
        "### 1.2 Dataset\n",
        "The dataset for the problem can be accessed in the following link:\n",
        "https://drive.google.com/drive/folders/1p6m9Do5d2qr51TWxW1xpVNQnd8Lq7uic?usp=sharing \n",
        "\n",
        "Divide it into train and valid splits based on your preference and finally form your folder structure as below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMSEpjPse2Rb"
      },
      "source": [
        "----------------------\n",
        "    Provide path to dataset\n",
        "    Format: PascalVOC\n",
        "\n",
        "                root\n",
        "         /     /            \\      \\ \n",
        "    train train_annots  valid  valid_annots\n",
        "\n",
        "  -----------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FCAAlGW6Bqu"
      },
      "source": [
        "- **The size of the dataset is what makes the problem interesting!**<br>\n",
        "You don't have a lot of data, thus your goal is to maximize learning on this scarce data.<br>\n",
        "Wonder how you can leverage your Deep learning skills to obtain decent results for the problem!\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjG5Ef9QXZnX"
      },
      "source": [
        "### 1.3 Suggestions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFx8OcIi6Bqw"
      },
      "source": [
        "Some pointers to help you: <br>\n",
        "1. Browse about existing techniques like few shot learning, and how they are used when the size of datasets is small.\n",
        "2. Try to learn about object detectors based on backbones like ResNet, which may be efficient in locating small objects.\n",
        "\n",
        "Feel free to come up with other interesting ideas!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRzvdZKKVD3R"
      },
      "source": [
        "## 2.  A Sample Detector\n",
        "This is an example notebook for the Cosmic Ray Challenge. In this notebook, we present an example of training an object detector called 'Region Proposal Network' (RPN) on this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RtZZoKZ9WFN"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torchvision.datasets.voc import VisionDataset\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if8Jy5sSCiW-",
        "outputId": "0a620da7-2f32-4050-8cd7-4f509fd08c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SopbBeDs-PTD",
        "outputId": "2615adf7-8825-4767-a0bf-6fd37a0ba8be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount your drive after you download and store the dataset in your drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MbkED_xWnKW"
      },
      "source": [
        "### 2.1 Dataset pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3nTo-5E-f2a"
      },
      "source": [
        "def generate_anchors(target): # data is the training set\n",
        "  gt_bbox = np.array(target[0],dtype=np.float32)\n",
        "  gt_labels = np.array(target[1])\n",
        "  anchor_areas = [ x**2 for x in [16.0, 32.0, 64.0]]\n",
        "  ratios = [0.5, 1, 2]\n",
        "  anchor_scales = [1]\n",
        "  fe_size = 32\n",
        "  anchors = np.zeros((len(anchor_areas)*len(ratios)*len(anchor_scales)*fe_size*fe_size, 4),dtype=np.float32)\n",
        "  ctr =  np.zeros((fe_size*fe_size, 2),dtype=np.float32)\n",
        "  pts = np.arange(1024/fe_size-1, 1024, 1024/fe_size)\n",
        "  index = 0\n",
        "  for x in range(len(pts)):\n",
        "      for y in range(len(pts)):\n",
        "          ctr[index, 0] = pts[x] - 1024/(fe_size*2)\n",
        "          ctr[index, 1] = pts[y] - 1024/(fe_size*2)\n",
        "          index +=1\n",
        "\n",
        "  # Code to generate anchors at a location in Fmap\n",
        "  index = 0\n",
        "  for c in ctr:\n",
        "    ctr_y, ctr_x = c\n",
        "    for i in range(len(ratios)):\n",
        "      for j in range(len(anchor_scales)):\n",
        "        for k in range(len(anchor_areas)):\n",
        "          h = anchor_scales[j] * np.sqrt(anchor_areas[k]/ratios[i])\n",
        "          w = anchor_scales[j] * (anchor_areas[k]/h)\n",
        "          anchors[index, 0] = ctr_y - h / 2.\n",
        "          anchors[index, 1] = ctr_x - w / 2.\n",
        "          anchors[index, 2] = ctr_y + h / 2.\n",
        "          anchors[index, 3] = ctr_x + w / 2.\n",
        "          index += 1\n",
        "  valid_anchors = anchors\n",
        "  valid_anchors[:, slice(0, 4, 2)] = np.clip(\n",
        "              anchors[:, slice(0, 4, 2)], 0, 1024)\n",
        "  valid_anchors[:, slice(1, 4, 2)] = np.clip(\n",
        "      anchors[:, slice(1, 4, 2)], 0, 1024)\n",
        "  # Create labels and anchors for valid anchor boxes\n",
        "  label = np.empty((len(valid_anchors), ), dtype=np.int32)\n",
        "  # default initialisation\n",
        "  label.fill(-1)\n",
        "\n",
        "  if (len(gt_labels)==0):\n",
        "    anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchors.dtype)\n",
        "    anchor_locations.fill(0)\n",
        "    return label,anchor_locations\n",
        "  ious = np.empty((len(valid_anchors), len(gt_labels)), dtype=np.float32)\n",
        "  ious.fill(0)\n",
        "\n",
        "# Calculating iou\n",
        "  for num1, i in enumerate(valid_anchors):\n",
        "      ya1, xa1, ya2, xa2 = i  \n",
        "      anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
        "      for num2, j in enumerate(gt_bbox):\n",
        "          yb1, xb1, yb2, xb2 = j\n",
        "          box_area = (yb2- yb1) * (xb2 - xb1)\n",
        "          inter_x1 = max([xb1, xa1])\n",
        "          inter_y1 = max([yb1, ya1])\n",
        "          inter_x2 = min([xb2, xa2])\n",
        "          inter_y2 = min([yb2, ya2])\n",
        "          if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
        "              iter_area = (inter_y2 - inter_y1)*(inter_x2 - inter_x1)\n",
        "              iou = iter_area/(anchor_area+ box_area - iter_area)            \n",
        "          else:\n",
        "              iou = 0\n",
        "          ious[num1, num2] = iou\n",
        "\n",
        "  gt_argmax_ious = ious.argmax(axis=0)\n",
        "  gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
        "  argmax_ious = ious.argmax(axis=1)\n",
        "  max_ious = ious[np.arange(len(label)), argmax_ious]\n",
        "\n",
        "  # LABELLING GT ANCHORS\n",
        "  pos_iou_threshold  = 0.5\n",
        "  neg_iou_threshold = 0.05\n",
        "  label[max_ious < neg_iou_threshold] = 0\n",
        "  label[gt_argmax_ious] = 1\n",
        "  label[max_ious >= pos_iou_threshold] = 1\n",
        "\n",
        "  # Creating 256 training samples,half positive, half negative\n",
        "  # pos_ratio = 0.5\n",
        "  n_neg = 1024\n",
        "  pos_index = np.where(label==1)[0]\n",
        "  neg_index = np.where(label == 0)[0]\n",
        "  if len(neg_index) > n_neg:\n",
        "      disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
        "      label[disable_index] = -1\n",
        "\n",
        "  final_neg_anchor_indices = np.where(label==0)[0]\n",
        "  final_neg_anchors = valid_anchors[final_neg_anchor_indices]\n",
        "# get the most probable gt_bbox at each valid anchor location \n",
        "  max_iou_bbox = gt_bbox[argmax_ious]\n",
        "\n",
        "  height = valid_anchors[:, 2] - valid_anchors[:, 0]\n",
        "  width = valid_anchors[:, 3] - valid_anchors[:, 1]\n",
        "  ctr_y = valid_anchors[:, 0] + 0.5 * height\n",
        "  ctr_x = valid_anchors[:, 1] + 0.5 * width\n",
        "  base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
        "  base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
        "  base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
        "  base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width\n",
        "  eps = np.finfo(height.dtype).eps\n",
        "  height = np.maximum(height, eps)\n",
        "  width = np.maximum(width, eps)\n",
        "  dy = (base_ctr_y - ctr_y) / height\n",
        "  dx = (base_ctr_x - ctr_x) / width\n",
        "  dh = np.log(base_height / height)\n",
        "  dw = np.log(base_width / width)\n",
        "  anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "  anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)\n",
        "  anchor_locations.fill(0)\n",
        "  anchor_locations = anchor_locs\n",
        "  # print(label.shape,anchor_locations.shape)\n",
        "  return label,anchor_locations\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwxorFvZ6Bqz"
      },
      "source": [
        "# data loader\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "}\n",
        "class Cosmic_Detection(VisionDataset):\n",
        "  def __init__(self,\n",
        "                root,                 \n",
        "                mode='train',\n",
        "                transform=None,\n",
        "                target_transform=None,\n",
        "                transforms=None):\n",
        "      super(Cosmic_Detection, self).__init__(root, transforms, transform, target_transform)\n",
        "\n",
        "      voc_root = 'root'  # Name of your dataset folder \n",
        "      image_dir = os.path.join(voc_root, mode)\n",
        "      annotation_dir = os.path.join(voc_root, mode +'_annots')\n",
        "\n",
        "      if not os.path.isdir(voc_root):\n",
        "          raise RuntimeError('Dataset not found or corrupted')\n",
        "\n",
        "      file_names = []\n",
        "      for f in os.listdir(image_dir):\n",
        "        file_names.append(f[:-4])\n",
        "      self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "      self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
        "      assert (len(self.images) == len(self.annotations))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      img = Image.open(self.images[index]).convert('RGB')\n",
        "      target = self.parse_voc_xml(ET.parse(self.annotations[index]).getroot())\n",
        "\n",
        "      if self.transforms is not None:\n",
        "          img, target = self.transforms(img, target)\n",
        "\n",
        "      labels, locations = generate_anchors(target)\n",
        "\n",
        "      return img.view(1,3,1024,1024), labels, locations\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "    \n",
        "  def parse_voc_xml(self, root):\n",
        "        # extract each bounding box\n",
        "        boxes = list()\n",
        "        for box in root.findall('.//bndbox'):\n",
        "            xmin = int(box.find('xmin').text)\n",
        "            ymin = int(box.find('ymin').text)\n",
        "            xmax = int(box.find('xmax').text)\n",
        "            ymax = int(box.find('ymax').text)\n",
        "            coors = [ymin, xmin, ymax, xmax]\n",
        "            boxes.append(coors)\n",
        "        # extract image dimensions\n",
        "        width = int(root.find('.//size/width').text)\n",
        "        height = int(root.find('.//size/height').text)\n",
        "        # extract all the classes\n",
        "        labels = list()\n",
        "        for category in root.findall('.//name'):\n",
        "          labels.append(category.text)\n",
        "        return boxes, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQz75QoOElgy"
      },
      "source": [
        "dataset = {x: Cosmic_Detection(None, x, data_transforms[x]) \n",
        "                  for x in ['train', 'valid']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size=1,\n",
        "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
        "              for x in ['train', 'valid']}\n",
        "\n",
        "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'valid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl3Yy27-EX-5",
        "outputId": "29615d60-5359-469b-af55-c458381f84bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for inputs,labels,locs in dataloaders['train']:\n",
        "        print(inputs[0].shape)\n",
        "        print(labels[0].shape)\n",
        "        print(locs[0].shape)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 1024, 1024])\n",
            "torch.Size([9216])\n",
            "torch.Size([9216, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDvcmFvbW9oB"
      },
      "source": [
        "### 2.2 Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtlOjWSy_7T4",
        "outputId": "48efd3b6-bf20-414c-be76-2d402e134252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "VGG_model = torchvision.models.vgg16(pretrained=True)\n",
        "fe = list(VGG_model.features)\n",
        "mid_channels = 512\n",
        "in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512\n",
        "n_anchor = 9 # Number of anchors at each location\n",
        "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
        "# Initialising the layer weights with values from Gaussian(0,0.01)\n",
        "# conv sliding layer\n",
        "conv1.weight.data.normal_(0, 0.01)\n",
        "conv1.bias.data.zero_()\n",
        "print(fe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S65jJMf9_32t"
      },
      "source": [
        "class Detector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Detector, self).__init__()\n",
        "        self.backbone = torch.nn.Sequential(*fe,conv1)\n",
        "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0)\n",
        "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0)\n",
        "        # Regression layer\n",
        "        self.reg_layer.weight.data.normal_(0, 0.01)\n",
        "        self.reg_layer.bias.data.zero_()\n",
        "        # classification layer\n",
        "        self.cls_layer.weight.data.normal_(0, 0.01)\n",
        "        self.cls_layer.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, input_image):\n",
        "        out_map = self.backbone(input_image)\n",
        "        pred_cls_scores = self.cls_layer(out_map)\n",
        "        pred_anchor_locs = self.reg_layer(out_map)\n",
        "        return pred_cls_scores, pred_anchor_locs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54YdeLWNCZGl"
      },
      "source": [
        "CR_Detector = Detector().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmtyPFbyZfHp"
      },
      "source": [
        "### 2.3 Example Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiW-gHRTC68u"
      },
      "source": [
        "def train_model(model, start_epoch = 0,num_epochs=1):\n",
        "  weights = [0.05, 0.95]\n",
        "  class_weights = torch.FloatTensor(weights).to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "  since = time.time()\n",
        "  model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_loss = 100.0\n",
        "        \n",
        "  for epoch in range(num_epochs):        \n",
        "      print('Epoch {}/{}'.format(epoch + start_epoch, start_epoch+ num_epochs-1))\n",
        "      print('-' * 10)\n",
        "\n",
        "      # Each epoch has a training and validation phase\n",
        "      for phase in ['train', 'valid']:\n",
        "          if phase == 'train':\n",
        "              model.train()  # Set model to training mode\n",
        "          else:\n",
        "              model.eval()   # Set model to evaluate mode\n",
        "                \n",
        "          cls_loss = 0.0\n",
        "          reg_loss = 0.0  \n",
        "          running_loss = 0.0            \n",
        "          total_cls_loss = 0.0\n",
        "          total_reg_loss = 0.0\n",
        "          total_epoch_loss = 0.0\n",
        "          index = -1\n",
        "\n",
        "          # Iterate over data.\n",
        "          for inputs, labels, locations in dataloaders[phase]:\n",
        "            # Batch size 1\n",
        "              index+=1\n",
        "              inputs = inputs[0].to(device)\n",
        "              target_labels_np = labels[0].data.numpy()\n",
        "              pos_arr = np.where(target_labels_np > 0)[0]\n",
        "              target_labels = labels[0].to(device)\n",
        "              target_locs = locations[0].to(device)\n",
        "              pos = target_labels > 0\n",
        "              # zero the parameter gradients\n",
        "              optimizer.zero_grad()\n",
        "              # forward\n",
        "              # track history if only in train\n",
        "              with torch.set_grad_enabled(phase == 'train'):\n",
        "                  pred_labels, pred_locs = model(inputs)\n",
        "                  if torch.isnan(pred_locs ).any():\n",
        "                      print(\"Nan in output prediction:\", index)\n",
        "                      return None\n",
        "\n",
        "                  pred_labels = pred_labels.permute(0, 2, 3, 1).contiguous()\n",
        "                  pred_labels = pred_labels.view(1, 32, 32, 9, 2).contiguous().view(-1, 2)\n",
        "                  pred_locs = pred_locs.permute(0, 2, 3, 1).contiguous().view(-1, 4)\n",
        "                  rpn_cls_loss = F.cross_entropy(pred_labels, target_labels.long(), ignore_index = -1,weight = class_weights)\n",
        "                \n",
        "                  if not(pos_arr.size == 0):\n",
        "                      mask = pos.unsqueeze(1).expand_as(pred_locs)\n",
        "                      mask_loc_preds = pred_locs[mask].view(-1,4)\n",
        "                      mask_loc_targets = target_locs[mask].view(-1, 4)\n",
        "                      x = torch.abs(mask_loc_targets - mask_loc_preds)\n",
        "                      rpn_loc_loss = (((x < 1).float() * 0.5 * x*x) + ((x >= 1).float() * (x-0.5))).sum()\n",
        "                      rpn_loc_loss_item = rpn_loc_loss.item()\n",
        "                      rpn_loss = rpn_cls_loss +  rpn_loc_loss\n",
        "                  else:\n",
        "                      rpn_loc_loss_item = 0\n",
        "                      rpn_loss = rpn_cls_loss\n",
        "                  \n",
        "                  # backward + optimize only if in training phase\n",
        "                  if phase == 'train':\n",
        "                      rpn_loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "              # statistics\n",
        "              cls_loss += rpn_cls_loss.item()\n",
        "              reg_loss += rpn_loc_loss_item\n",
        "              running_loss += rpn_loss.item()\n",
        "          # if phase == 'train':\n",
        "          #     scheduler.step()\n",
        "\n",
        "          total_cls_loss = cls_loss / dataset_sizes[phase]\n",
        "          total_reg_loss = reg_loss / dataset_sizes[phase]\n",
        "          total_epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "          print('{} rpn_cls Loss: {:.4f}       {} rpn_reg Loss: {:.4f}       {} Total Loss: {:.4f}'.format(phase, total_cls_loss,phase, total_reg_loss,phase, total_epoch_loss))\n",
        "\n",
        "          # deep copy the model\n",
        "          if phase == 'train' and total_epoch_loss < best_loss:\n",
        "              best_loss = total_epoch_loss\n",
        "              model_wts = copy.deepcopy(model.state_dict())\n",
        "      print()\n",
        "  \n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "      time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "  # load model weights\n",
        "  model.load_state_dict(model_wts)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4BT2A0SDU1m",
        "outputId": "373952af-0832-43d2-8b6d-04460133d2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Example Training for 1 epoch\n",
        "model_1 = train_model(CR_Detector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "train rpn_cls Loss: 0.5182       train rpn_reg Loss: 0.9223       train Total Loss: 1.4404\n",
            "valid rpn_cls Loss: 0.4053       valid rpn_reg Loss: 0.7235       valid Total Loss: 1.1288\n",
            "\n",
            "Training complete in 2m 35s\n",
            "Best Loss: 1.440448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbADW8Js6Bq9"
      },
      "source": [
        "## 3. Evaluation and Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcF4LTaaZq1s"
      },
      "source": [
        "### 3.1 Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd0h4bZCY4BH"
      },
      "source": [
        "- You will be evaluated based on the following criteria:\n",
        "  - **Mean Average Precision (MAP) score**, which is a standard metric to evaluate object detection models (80% weightage)\n",
        "  - Your idea to approach this problem, basically to judge how well you have internalized the problem!  (20% weightage)\n",
        "\n",
        "- You are encouraged to implement the evaluation module yourself. If not, there are tons of repositories to help you which provide you the implementation of these metrics: Just google 'MAP Object detection git'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA94yVjEYv0T"
      },
      "source": [
        "### 3.2 Submission Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiRDG2xm6Bq_"
      },
      "source": [
        "You need to submit the predictions of your model on the test data as follows:\n",
        "\n",
        "- For each image in the test folder, you need to write your predictions into a corresponding \".txt\" file with the same image name, following the format below:<br>\n",
        "   ```Positive  <confidence> <left> <top> <right> <bottom>```\n",
        "    - Class name - Positive (will be constant across all predictions)\n",
        "    - Confidence score for that prediction (between 0 and 1)\n",
        "    - left x co-ordinate of the box predicted (between 0 and 1024)\n",
        "    - top y co-ordinate of the box predicted (between 0 and 1024)\n",
        "    - right x co-ordinate of the box predicted (between 0 and 1024)\n",
        "    - bottom y co-ordinate of the box predicted (between 0 and 1024)\n",
        "    \n",
        "- An image may contain more than 1 artifact, in which case each prediction instance is to be written on a separate line of the '.txt' file.\n",
        "- Put all these '.txt' files into a single folder with the name ```predictions/``` \n",
        "- Your submission should include the ```notebook``` along with the ```predictions/``` folder. Zip them into ```<Your Team Number>.zip``` before submission.\n",
        "    \n",
        "    \n",
        "    \n",
        "\n"
      ]
    }
  ]
}